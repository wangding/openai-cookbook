{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PP-vMETEu8R"
      },
      "source": [
        "# 使用 Chroma 和 OpenAI 实现强健的问答系统\n",
        "\n",
        "本笔记本将逐步指导您使用 [Chroma](https://trychroma.com)，一个开源的嵌入式数据库，以及 OpenAI 的 [文本嵌入](https://platform.openai.com/docs/guides/embeddings/use-cases) 和 [聊天完成](https://platform.openai.com/docs/guides/chat) API 来回答关于一组数据的问题。\n",
        "\n",
        "此外，本笔记本演示了使问答系统更加强健的一些权衡。正如我们将看到的，*简单的查询并不总是能够产生最佳结果*！\n",
        "\n",
        "## 带 LLM 的问答\n",
        "\n",
        "像 OpenAI 的 ChatGPT 这样的大语言模型（LLM）可以用来回答关于模型可能没有经过训练或接触的数据的问题。例如：\n",
        "\n",
        "- 个人数据，如电子邮件和笔记\n",
        "- 高度专业化的数据，如档案或法律文件\n",
        "- 新创建的数据，如最新新闻报道\n",
        "\n",
        "为了克服这个限制，我们可以使用一个与 LLM 本身一样易于用自然语言查询的数据存储。像 Chroma 这样的嵌入式存储将文档表示为 [嵌入](https://openai.com/blog/introducing-text-and-code-embeddings) 和文档本身一起。\n",
        "\n",
        "通过嵌入文本查询，Chroma 可以找到相关的文档，然后我们可以将这些文档传递给 LLM 来回答我们的问题。我们将展示这种方法的详细示例和变体。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hykesLtIEu8W"
      },
      "source": [
        "# 配置和准备工作\n",
        "\n",
        "首先，确保我们需要的 Python 依赖已安装。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7j2D3nJEu8X"
      },
      "outputs": [],
      "source": [
        "%pip install -qU openai chromadb pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdnTlBUeEu8Y"
      },
      "source": [
        "本笔记本始终使用 OpenAI 的 API。您可以从 [https://beta.openai.com/account/api-keys](https://beta.openai.com/account/api-keys) 获取 API 密钥。\n",
        "\n",
        "您可以通过在终端中执行命令 `export OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx` 将您的 API 密钥添加为环境变量。请注意，如果环境变量还没有设置，您需要重新加载笔记本。或者，您可以在笔记本中设置它，请参见下文。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohWk-UQVEu8Y",
        "outputId": "5bdcb639-4447-4fb5-a0fe-a45cfba32a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY is ready\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Uncomment the following line to set the environment variable in the notebook\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" \n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
        "    print(\"OPENAI_API_KEY is ready\")\n",
        "    import openai\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY environment variable not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri9w2n6uEu8Z"
      },
      "source": [
        "# 数据集\n",
        "\n",
        "在本笔记本中，我们使用 [SciFact 数据集](https://github.com/allenai/scifact)。这是一个精心策划的数据集，由专家注释的科学说法，附带论文标题和摘要的文本语料库。根据语料库中的文档，每个主张可能得到支持、反驳或没有足够的证据。\n",
        "\n",
        "有了语料库作为基础事实，我们可以研究以下方法对 LLM 问答的表现如何。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoCzTnASEu8Z",
        "outputId": "96b50999-a52e-49df-de38-cc09e1f9d425"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>claim</th>\n",
              "      <th>evidence</th>\n",
              "      <th>cited_doc_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0-dimensional biomaterials show inductive prop...</td>\n",
              "      <td>{}</td>\n",
              "      <td>[31715818]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1,000 genomes project enables mapping of genet...</td>\n",
              "      <td>{'14717500': [{'sentences': [2, 5], 'label': '...</td>\n",
              "      <td>[14717500]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1/2000 in UK have abnormal PrP positivity.</td>\n",
              "      <td>{'13734012': [{'sentences': [4], 'label': 'SUP...</td>\n",
              "      <td>[13734012]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>5% of perinatal mortality is due to low birth ...</td>\n",
              "      <td>{}</td>\n",
              "      <td>[1606628]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36</td>\n",
              "      <td>A deficiency of vitamin B12 increases blood le...</td>\n",
              "      <td>{}</td>\n",
              "      <td>[5152028, 11705328]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                              claim  \\\n",
              "0   1  0-dimensional biomaterials show inductive prop...   \n",
              "1   3  1,000 genomes project enables mapping of genet...   \n",
              "2   5         1/2000 in UK have abnormal PrP positivity.   \n",
              "3  13  5% of perinatal mortality is due to low birth ...   \n",
              "4  36  A deficiency of vitamin B12 increases blood le...   \n",
              "\n",
              "                                            evidence        cited_doc_ids  \n",
              "0                                                 {}           [31715818]  \n",
              "1  {'14717500': [{'sentences': [2, 5], 'label': '...           [14717500]  \n",
              "2  {'13734012': [{'sentences': [4], 'label': 'SUP...           [13734012]  \n",
              "3                                                 {}            [1606628]  \n",
              "4                                                 {}  [5152028, 11705328]  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the claim dataset\n",
        "import pandas as pd\n",
        "\n",
        "data_path = '../../data'\n",
        "\n",
        "claim_df = pd.read_json(f'{data_path}/scifact_claims.jsonl', lines=True)\n",
        "claim_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0_fcXOEu8a"
      },
      "source": [
        "# 直接向模型提问\n",
        "\n",
        "GPT-3.5 通过大量的科学信息进行训练。作为基准，我们想了解模型在没有任何其他上下文的情况下已经知道了什么。这将允许我们校准整体性能。\n",
        "\n",
        "我们构建适当的提示，带有一些示例事实，然后查询数据集中的每个声明。我们要求模型将声明评估为“真”，“假”或“NEE”，如果没有足够的证据支持任何一方。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEiDyMR0Eu8a"
      },
      "outputs": [],
      "source": [
        "def build_prompt(claim):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"I will ask you to assess a scientific claim. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"        \n",
        "Example:\n",
        "\n",
        "Claim:\n",
        "0-dimensional biomaterials show inductive properties.\n",
        "\n",
        "Assessment:\n",
        "False\n",
        "\n",
        "Claim:\n",
        "1/2000 in UK have abnormal PrP positivity.\n",
        "\n",
        "Assessment:\n",
        "True\n",
        "\n",
        "Claim:\n",
        "Aspirin inhibits the production of PGE2.\n",
        "\n",
        "Assessment:\n",
        "False\n",
        "\n",
        "End of examples. Assess the following claim:\n",
        "\n",
        "Claim:\n",
        "{claim}\n",
        "\n",
        "Assessment:\n",
        "\"\"\"}\n",
        "    ]\n",
        "\n",
        "\n",
        "def assess_claims(claims):\n",
        "    responses = []\n",
        "    # Query the OpenAI API\n",
        "    for claim in claims:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=build_prompt(claim),\n",
        "            max_tokens=3,\n",
        "        )\n",
        "        # Strip any punctuation or whitespace from the response\n",
        "        responses.append(response.choices[0].message.content.strip('., '))\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPGkEhQmEu8b"
      },
      "source": [
        "我们从数据集中随机选择了 100 个声明。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho6UAO6zEu8b"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at 100 claims\n",
        "samples = claim_df.sample(50)\n",
        "\n",
        "claims = samples['claim'].tolist() \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVx7RLFZEu8c"
      },
      "source": [
        "我们根据数据集评估基本事实。根据数据集的描述，每个声明要么被证据支持，要么被证据否定，要么没有足够的证据支持任何一方。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe28MeQrEu8c"
      },
      "outputs": [],
      "source": [
        "def get_groundtruth(evidence):\n",
        "    groundtruth = []\n",
        "    for e in evidence:\n",
        "        # Evidence is empty \n",
        "        if len(e) == 0:\n",
        "            groundtruth.append('NEE')\n",
        "        else:\n",
        "            # In this dataset, all evidence for a given claim is consistent, either SUPPORT or CONTRADICT\n",
        "            if list(e.values())[0][0]['label'] == 'SUPPORT':\n",
        "                groundtruth.append('True')\n",
        "            else:\n",
        "                groundtruth.append('False')\n",
        "    return groundtruth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vka2CoueEu8c"
      },
      "outputs": [],
      "source": [
        "evidence = samples['evidence'].tolist()\n",
        "groundtruth = get_groundtruth(evidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9xilGCEu8c"
      },
      "source": [
        "我们还输出混淆矩阵，比较模型的评估和基本事实，并以易于阅读的表格形式展示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92kK1qlLEu8c"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(inferred, groundtruth):\n",
        "    assert len(inferred) == len(groundtruth)\n",
        "    confusion = {\n",
        "        'True': {'True': 0, 'False': 0, 'NEE': 0},\n",
        "        'False': {'True': 0, 'False': 0, 'NEE': 0},\n",
        "        'NEE': {'True': 0, 'False': 0, 'NEE': 0},\n",
        "    }\n",
        "    for i, g in zip(inferred, groundtruth):\n",
        "        confusion[i][g] += 1\n",
        "\n",
        "    # Pretty print the confusion matrix\n",
        "    print('\\tGroundtruth')\n",
        "    print('\\tTrue\\tFalse\\tNEE')\n",
        "    for i in confusion:\n",
        "        print(i, end='\\t')\n",
        "        for g in confusion[i]:\n",
        "            print(confusion[i][g], end='\\t')\n",
        "        print()\n",
        "\n",
        "    return confusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qVdtucqEu8d"
      },
      "source": [
        "我们要求模型直接评估声明，没有其他上下文。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vli1c_pwEu8d",
        "outputId": "fa10cd6f-815e-4e13-a255-1f55233334df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tGroundtruth\n",
            "\tTrue\tFalse\tNEE\n",
            "True\t15\t5\t14\t\n",
            "False\t0\t2\t1\t\n",
            "NEE\t3\t3\t7\t\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'True': {'True': 15, 'False': 5, 'NEE': 14},\n",
              " 'False': {'True': 0, 'False': 2, 'NEE': 1},\n",
              " 'NEE': {'True': 3, 'False': 3, 'NEE': 7}}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_inferred = assess_claims(claims)\n",
        "confusion_matrix(gpt_inferred, groundtruth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1TQmR4oEu8d"
      },
      "source": [
        "## 结果\n",
        "\n",
        "从这些结果中，我们看到 LLM 强烈倾向于将声明评估为真，即使它们是假的，并且也倾向于将假的声明评估为没有足够的证据。请注意，“没有足够的证据”是针对模型在真空环境下对声明的评估，而没有其他上下文。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm7diXCbEu8d"
      },
      "source": [
        "# 添加上下文\n",
        "\n",
        "现在，我们将可用的语料库标题和摘要的附加上下文添加进来。本节将展示如何使用 OpenAI 文本嵌入将文本语料库加载到 Chroma 中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEjVcfgREu8d"
      },
      "source": [
        "First, we load the text corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "438x9t8BEu8e",
        "outputId": "30a544a8-aaa7-44fc-e732-d554de455f17"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>structured</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4983</td>\n",
              "      <td>Microstructural development of human newborn c...</td>\n",
              "      <td>[Alterations of the architecture of cerebral w...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5836</td>\n",
              "      <td>Induction of myelodysplasia by myeloid-derived...</td>\n",
              "      <td>[Myelodysplastic syndromes (MDS) are age-depen...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7912</td>\n",
              "      <td>BC1 RNA, the transcript from a master gene for...</td>\n",
              "      <td>[ID elements are short interspersed elements (...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18670</td>\n",
              "      <td>The DNA Methylome of Human Peripheral Blood Mo...</td>\n",
              "      <td>[DNA methylation plays an important role in bi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19238</td>\n",
              "      <td>The human myelin basic protein gene is include...</td>\n",
              "      <td>[Two human Golli (for gene expressed in the ol...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doc_id                                              title  \\\n",
              "0    4983  Microstructural development of human newborn c...   \n",
              "1    5836  Induction of myelodysplasia by myeloid-derived...   \n",
              "2    7912  BC1 RNA, the transcript from a master gene for...   \n",
              "3   18670  The DNA Methylome of Human Peripheral Blood Mo...   \n",
              "4   19238  The human myelin basic protein gene is include...   \n",
              "\n",
              "                                            abstract  structured  \n",
              "0  [Alterations of the architecture of cerebral w...       False  \n",
              "1  [Myelodysplastic syndromes (MDS) are age-depen...       False  \n",
              "2  [ID elements are short interspersed elements (...       False  \n",
              "3  [DNA methylation plays an important role in bi...       False  \n",
              "4  [Two human Golli (for gene expressed in the ol...       False  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the corpus into a dataframe\n",
        "corpus_df = pd.read_json(f'{data_path}/scifact_corpus.jsonl', lines=True)\n",
        "corpus_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOFYlv55Eu8e"
      },
      "source": [
        "## 将语料库加载到 Chroma 中\n",
        "\n",
        "下一步是将语料库加载到 Chroma 中。给定嵌入函数，Chroma 将自动处理嵌入每个文档，并将其存储在其文本和元数据旁边，使查询变得简单。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eyA2_6KEu8e"
      },
      "source": [
        "We instantiate a (ephemeral) Chroma client, and create a collection for the SciFact title and abstract corpus. \n",
        "Chroma can also be instantiated in a persisted configuration; learn more at the [Chroma docs](https://docs.trychroma.com/usage-guide?lang=py). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQdPudUWEu8e",
        "outputId": "d16cfc30-94f8-45b0-9466-44896a8c1604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Chroma using direct local API.\n",
            "Using DuckDB in-memory for database. Data will be transient.\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "\n",
        "# We initialize an embedding function, and provide it to the collection.\n",
        "embedding_function = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "scifact_corpus_collection = chroma_client.create_collection(name='scifact_corpus', embedding_function=embedding_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV5TOdIdEu8e"
      },
      "source": [
        "Next we load the corpus into Chroma. Because this data loading is memory intensive, we recommend using a batched loading scheme in batches of 50-1000. For this example it should take just over one minute for the entire corpus. It's being embedded in the background, automatically, using the `embedding_function` we specified earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLeE5c_wEu8e"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "\n",
        "for i in range(0, len(corpus_df), batch_size):\n",
        "    batch_df = corpus_df[i:i+batch_size]\n",
        "    scifact_corpus_collection.add(\n",
        "        ids=batch_df['doc_id'].apply(lambda x: str(x)).tolist(), # Chroma takes string IDs.\n",
        "        documents=(batch_df['title'] + '. ' + batch_df['abstract'].apply(lambda x: ' '.join(x))).to_list(), # We concatenate the title and abstract.\n",
        "        metadatas=[{\"structured\": structured} for structured in batch_df['structured'].to_list()] # We also store the metadata, though we don't use it in this example.\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL3_Em1LEu8e"
      },
      "source": [
        "## Retrieving context\n",
        "\n",
        "Next we retrieve documents from the corpus which may be relevant to each claim in our sample. We want to provide these as context to the LLM for evaluating the claims. We retrieve the 3 most relevant documents for each claim, according to the embedding distance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfYSwdUKEu8f"
      },
      "outputs": [],
      "source": [
        "claim_query_result = scifact_corpus_collection.query(query_texts=claims, include=['documents', 'distances'], n_results=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuvy_fLBEu8f"
      },
      "source": [
        "We create a new prompt, this time taking into account the additional context we retrieve from the corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi-ItL2WEu8f"
      },
      "outputs": [],
      "source": [
        "def build_prompt_with_context(claim, context):\n",
        "    return [{'role': 'system', 'content': \"I will ask you to assess whether a particular scientific claim, based on evidence provided. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"}, \n",
        "            {'role': 'user', 'content': f\"\"\"\"\n",
        "The evidence is the following:\n",
        "\n",
        "{' '.join(context)}\n",
        "\n",
        "Assess the following claim on the basis of the evidence. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence. Do not output any other text. \n",
        "\n",
        "Claim:\n",
        "{claim}\n",
        "\n",
        "Assessment:\n",
        "\"\"\"}]\n",
        "\n",
        "\n",
        "def assess_claims_with_context(claims, contexts):\n",
        "    responses = []\n",
        "    # Query the OpenAI API\n",
        "    for claim, context in zip(claims, contexts):\n",
        "        # If no evidence is provided, return NEE\n",
        "        if len(context) == 0:\n",
        "            responses.append('NEE')\n",
        "            continue\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=build_prompt_with_context(claim=claim, context=context),\n",
        "            max_tokens=3,\n",
        "        )\n",
        "        # Strip any punctuation or whitespace from the response\n",
        "        responses.append(response.choices[0].message.content.strip('., '))\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR7A1yjtEu8f"
      },
      "source": [
        "Then ask the model to evaluate the claims with the retrieved context. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24fBEGwqEu8f",
        "outputId": "c81de2a9-14ef-4818-cc35-e794ad36fa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tGroundtruth\n",
            "\tTrue\tFalse\tNEE\n",
            "True\t16\t2\t8\t\n",
            "False\t1\t6\t5\t\n",
            "NEE\t1\t2\t9\t\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'True': {'True': 16, 'False': 2, 'NEE': 8},\n",
              " 'False': {'True': 1, 'False': 6, 'NEE': 5},\n",
              " 'NEE': {'True': 1, 'False': 2, 'NEE': 9}}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_with_context_evaluation = assess_claims_with_context(claims, claim_query_result['documents'])\n",
        "confusion_matrix(gpt_with_context_evaluation, groundtruth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt_JS_OlEu8f"
      },
      "source": [
        "## Results\n",
        "\n",
        "We see that the model is a lot less likely to evaluate a False claim as true (2 instances VS 5 previously), but that claims without enough evidence are still often assessed as True or False.\n",
        "\n",
        "Taking a look at the retrieved documents, we see that they are sometimes not relevant to the claim - this causes the model to be confused by the extra information, and it may decide that sufficient evidence is present, even when the information is irrelevant. This happens because we always ask for the 3 'most' relevant documents, but these might not be relevant at all beyond a certain point.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40_fvCrlEu8f"
      },
      "source": [
        "## Filtering context on relevance\n",
        "\n",
        "Along with the documents themselves, Chroma returns a distance score. We can try thresholding on distance, so that fewer irrelevant documents make it into the context we provide the model. \n",
        "\n",
        "If, after filtering on the threshold, no context documents remain, we bypass the model and simply return that there is not enough evidence. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUxYKoB2Eu8g"
      },
      "outputs": [],
      "source": [
        "def filter_query_result(query_result, distance_threshold=0.25):\n",
        "# For each query result, retain only the documents whose distance is below the threshold\n",
        "    for ids, docs, distances in zip(query_result['ids'], query_result['documents'], query_result['distances']):\n",
        "        for i in range(len(ids)-1, -1, -1):\n",
        "            if distances[i] > distance_threshold:\n",
        "                ids.pop(i)\n",
        "                docs.pop(i)\n",
        "                distances.pop(i)\n",
        "    return query_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTBsjXGXEu8g"
      },
      "outputs": [],
      "source": [
        "filtered_claim_query_result = filter_query_result(claim_query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J24mZei-Eu8g"
      },
      "source": [
        "Now we assess the claims using this cleaner context. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sHRRejPEu8g",
        "outputId": "9c2a4fbc-ec37-4403-f5af-5d3f7ef874c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tGroundtruth\n",
            "\tTrue\tFalse\tNEE\n",
            "True\t10\t2\t1\t\n",
            "False\t0\t2\t1\t\n",
            "NEE\t8\t6\t20\t\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'True': {'True': 10, 'False': 2, 'NEE': 1},\n",
              " 'False': {'True': 0, 'False': 2, 'NEE': 1},\n",
              " 'NEE': {'True': 8, 'False': 6, 'NEE': 20}}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_with_filtered_context_evaluation = assess_claims_with_context(claims, filtered_claim_query_result['documents'])\n",
        "confusion_matrix(gpt_with_filtered_context_evaluation, groundtruth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWJsSUBTEu8g"
      },
      "source": [
        "## Results\n",
        "\n",
        "The model now assesses many fewer claims as True or False when there is not enough evidence present. However, it now biases away from certainty. Most claims are now assessed as having not enough evidence, because a large fraction of them are filtered out by the distance threshold. It's possible to tune the distance threshold to find the optimal operating point, but this can be difficult, and is dataset and embedding model dependent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBxCA_nSEu8g"
      },
      "source": [
        "# Hypothetical Document Embeddings: Using hallucinations productively\n",
        "\n",
        "We want to be able to retrieve relevant documents, without retrieving less relevant ones which might confuse the model. One way to accomplish this is to improve the retrieval query. \n",
        "\n",
        "Until now, we have queried the dataset using _claims_ which are single sentence statements, while the corpus contains _abstracts_ describing a scientific paper. Intuitively, while these might be related, there are significant differences in their structure and meaning. These differences are encoded by the embedding model, and so influence the distances between the query and the most relevant results. \n",
        "\n",
        "We can overcome this by leveraging the power of LLMs to generate relevant text. While the facts might be hallucinated, the content and structure of the documents the models generate is more similar to the documents in our corpus, than the queries are. This could lead to better queries and hence better results. \n",
        "\n",
        "This approach is called [Hypothetical Document Embeddings (HyDE)](https://arxiv.org/abs/2212.10496), and has been shown to be quite good at the retrieval task. It should help us bring more relevant information into the context, without polluting it.  \n",
        "\n",
        "TL;DR:\n",
        "- you get much better matches when you embed whole abstracts rather than single sentences\n",
        "- but claims are usually single sentences\n",
        "- So HyDE shows that using GPT3 to expand claims into hallucinated abstracts and then searching based on those abstracts works (claims -> abstracts -> results) better than searching directly (claims -> results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvw5qtCOEu8g"
      },
      "source": [
        "First, we use in-context examples to prompt the model to generate documents similar to what's in the corpus, for each claim we want to assess. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N-H3G4dEu8h"
      },
      "outputs": [],
      "source": [
        "def build_hallucination_prompt(claim):\n",
        "    return [{'role': 'system', 'content': \"\"\"I will ask you to write an abstract for a scientific paper which supports or refutes a given claim. It should be written in scientific language, include a title. Output only one abstract, then stop.\n",
        "    \n",
        "    An Example:\n",
        "\n",
        "    Claim:\n",
        "    A high microerythrocyte count raises vulnerability to severe anemia in homozygous alpha (+)- thalassemia trait subjects.\n",
        "\n",
        "    Abstract:\n",
        "    BACKGROUND The heritable haemoglobinopathy alpha(+)-thalassaemia is caused by the reduced synthesis of alpha-globin chains that form part of normal adult haemoglobin (Hb). Individuals homozygous for alpha(+)-thalassaemia have microcytosis and an increased erythrocyte count. Alpha(+)-thalassaemia homozygosity confers considerable protection against severe malaria, including severe malarial anaemia (SMA) (Hb concentration < 50 g/l), but does not influence parasite count. We tested the hypothesis that the erythrocyte indices associated with alpha(+)-thalassaemia homozygosity provide a haematological benefit during acute malaria.   \n",
        "    METHODS AND FINDINGS Data from children living on the north coast of Papua New Guinea who had participated in a case-control study of the protection afforded by alpha(+)-thalassaemia against severe malaria were reanalysed to assess the genotype-specific reduction in erythrocyte count and Hb levels associated with acute malarial disease. We observed a reduction in median erythrocyte count of approximately 1.5 x 10(12)/l in all children with acute falciparum malaria relative to values in community children (p < 0.001). We developed a simple mathematical model of the linear relationship between Hb concentration and erythrocyte count. This model predicted that children homozygous for alpha(+)-thalassaemia lose less Hb than children of normal genotype for a reduction in erythrocyte count of >1.1 x 10(12)/l as a result of the reduced mean cell Hb in homozygous alpha(+)-thalassaemia. In addition, children homozygous for alpha(+)-thalassaemia require a 10% greater reduction in erythrocyte count than children of normal genotype (p = 0.02) for Hb concentration to fall to 50 g/l, the cutoff for SMA. We estimated that the haematological profile in children homozygous for alpha(+)-thalassaemia reduces the risk of SMA during acute malaria compared to children of normal genotype (relative risk 0.52; 95% confidence interval [CI] 0.24-1.12, p = 0.09).   \n",
        "    CONCLUSIONS The increased erythrocyte count and microcytosis in children homozygous for alpha(+)-thalassaemia may contribute substantially to their protection against SMA. A lower concentration of Hb per erythrocyte and a larger population of erythrocytes may be a biologically advantageous strategy against the significant reduction in erythrocyte count that occurs during acute infection with the malaria parasite Plasmodium falciparum. This haematological profile may reduce the risk of anaemia by other Plasmodium species, as well as other causes of anaemia. Other host polymorphisms that induce an increased erythrocyte count and microcytosis may confer a similar advantage.\n",
        "\n",
        "    End of example. \n",
        "    \n",
        "    \"\"\"}, {'role': 'user', 'content': f\"\"\"\"\n",
        "    Perform the task for the following claim.\n",
        "\n",
        "    Claim:\n",
        "    {claim}\n",
        "\n",
        "    Abstract:\n",
        "    \"\"\"}]\n",
        "\n",
        "\n",
        "def hallucinate_evidence(claims):\n",
        "    # Query the OpenAI API\n",
        "    responses = []\n",
        "    # Query the OpenAI API\n",
        "    for claim in claims:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model='gpt-3.5-turbo',\n",
        "            messages=build_hallucination_prompt(claim),\n",
        "        )\n",
        "        responses.append(response.choices[0].message.content)\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBc635bjEu8h"
      },
      "source": [
        "We hallucinate a document for each claim.\n",
        "\n",
        "*NB: This can take a while, about 30m for 100 claims*. You can reduce the number of claims we want to assess to get results more quickly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztjQXe2XEu8h"
      },
      "outputs": [],
      "source": [
        "hallucinated_evidence = hallucinate_evidence(claims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CawcCJKGEu8h"
      },
      "source": [
        "We use the hallucinated documents as queries into the corpus, and filter the results using the same distance threshold. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnES-6m2Eu8h"
      },
      "outputs": [],
      "source": [
        "hallucinated_query_result = scifact_corpus_collection.query(query_texts=hallucinated_evidence, include=['documents', 'distances'], n_results=3)\n",
        "filtered_hallucinated_query_result = filter_query_result(hallucinated_query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izp5onl0Eu8h"
      },
      "source": [
        "We then ask the model to assess the claims, using the new context. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHqKrMEXEu8h",
        "outputId": "d5dc7135-2a49-4d64-aedd-daba4d83ff52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tGroundtruth\n",
            "\tTrue\tFalse\tNEE\n",
            "True\t15\t2\t5\t\n",
            "False\t1\t5\t4\t\n",
            "NEE\t2\t3\t13\t\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'True': {'True': 15, 'False': 2, 'NEE': 5},\n",
              " 'False': {'True': 1, 'False': 5, 'NEE': 4},\n",
              " 'NEE': {'True': 2, 'False': 3, 'NEE': 13}}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_with_hallucinated_context_evaluation = assess_claims_with_context(claims, filtered_hallucinated_query_result['documents'])\n",
        "confusion_matrix(gpt_with_hallucinated_context_evaluation, groundtruth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcSd8nSDEu8h"
      },
      "source": [
        "## Results\n",
        "\n",
        "Combining HyDE with a simple distance threshold leads to a significant improvement. The model no longer biases assessing claims as True, nor toward their not being enough evidence. It also correctly assesses when there isn't enough evidence more often."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_up7_iEu8i"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Equipping LLMs with a context based on a corpus of documents is a powerful technique for bringing the general reasoning and natural language interactions of LLMs to your own data. However, it's important to know that naive query and retrieval may not produce the best possible results! Ultimately understanding the data will help get the most out of the retrieval based question-answering approach. \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd16a328ca3d68029457069b79cb0b38eb39a0f5ccc4fe4473d3047707df8207"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}